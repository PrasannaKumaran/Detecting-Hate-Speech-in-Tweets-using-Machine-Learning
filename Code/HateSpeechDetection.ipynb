{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HATE SPEECH DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kshitij Sharma (185001080)\n",
    "* Prasannakumaran D (185001110)\n",
    "* Praveen kumar (185001113)\n",
    "* Sai Ashish (185001130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every year people are being scammed and bullied online by Russian hackers who spread fake news through twitter. Our objective is to identify such malicious tweets and categorize them based on the the toxicity level of the tweet. In our proposed methodology we consider using Convolutional Neural Networks to train our deep learning model to identify such harmful tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dabe919918366f3281a3253768b4c4693d05216d"
   },
   "source": [
    "### Proposed Methodology \n",
    "We first train a simple Convolutional Neural Network model to recognize various types of hate speech by word patterns using the data obtained from the Kaggle. \n",
    "We then apply it to the tweets associated with the bots of the Internet Research Agency (IRA) of Russia to try and characterize them and see if this matches well and if the hate speech model gives us any insights into the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1b26cf41-5330-41fa-b4ce-76447f8e1e2f",
    "_uuid": "1b8da79bd7896c54199ccf641c6b730b798fbd0e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import dask.dataframe as ddf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from IPython.display import Markdown, display\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "display_markdown = lambda x: display(Markdown(x))\n",
    "dmc = DummyClassifier()\n",
    "lrm = LogisticRegression()\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "022343c0-6cc6-4079-a3f2-4b89a7d5e0fe",
    "_uuid": "d5fa68b8d9f20c8ceb662df489662db5e6b78846"
   },
   "source": [
    "## Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3b2bf13-dccb-4b2b-82b0-86295ac0ee23",
    "_uuid": "9182d060cfbbcd070b67e5dbc1136c98a4262cf9"
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"./toxic/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the training data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3b2bf13-dccb-4b2b-82b0-86295ac0ee23",
    "_uuid": "9182d060cfbbcd070b67e5dbc1136c98a4262cf9"
   },
   "outputs": [],
   "source": [
    "train_text = train_data[\"comment_text\"].fillna(\"Invalid\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_data[list_classes].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8faf48b7-965e-49c0-9994-b5bae9650077",
    "_uuid": "14ac14dab11eab6430d4c8f8946bb0ce1b6f5ceb"
   },
   "source": [
    "## Sequence Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textual data cannot be interpreted by the machine and therefore the words, punctuations are tokenized using the tokenizer class. This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit_on_texts -- Updates internal vocabulary based on a list of texts\n",
    "* texts_to_sequences -- Transforms each text in texts to a sequence of integers\n",
    "* pad_sequences -- Since the sentences are not of the same length the sparse matrix is padded with zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0296763-d6e7-49cd-bdb5-33bb7f5c231d",
    "_uuid": "b53b6a50bb03f98e1605e5549f532503d7ef8d1d"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_text))\n",
    "tokenized_train_text = tokenizer.texts_to_sequences(train_text)\n",
    "X_train = sequence.pad_sequences(tokenized_train_text, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding : Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "* Dropout : Randomly sets elements to zero to prevent overfitting\n",
    "* Conv1D : 1D convolution layer\n",
    "* Dilation rate : Dilated convolutions introduce another parameter to convolutional layers called the dilation rate. This defines a spacing between the values in a kernel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "026bf4f6-00b0-4178-9bcc-fb8cbc44d1e0",
    "_uuid": "b334ddbf78cf4065be1969513bf0f9f10e6da643",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_model(max_dilation_rate = 4):\n",
    "    '''\n",
    "    Builds the neural network. The network implements Dropout regularization with a dropout rate of 25 %, \n",
    "    Kernel size of 3 and embedding size of 128, implements Binary crossentropy loss function, Adam Optimizer and \n",
    "    the choice of metric is binary accuracy\n",
    "    Input : conv_layers (Integer), max_dilation_rate (Integer)\n",
    "    Output: Returns the built model\n",
    "    '''\n",
    "    embed_size = 128\n",
    "    inp = Input(shape = (maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2 * embed_size, kernel_size = 3)(x)\n",
    "    prefilt_x = Conv1D(2 * embed_size, kernel_size = 3)(x)\n",
    "    out_conv = []\n",
    "    \n",
    "    for dilation_rate in range(max_dilation_rate):\n",
    "        x = prefilt_x\n",
    "        for i in range(3):\n",
    "            x = Conv1D(32*2**(i), kernel_size = 3, dilation_rate = 2 ** dilation_rate)(x)    \n",
    "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "        \n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aec3c661-33ce-44b7-8309-97da1653eadf",
    "_uuid": "8ae9fc16a72d57f707012d35ea5db3d538dbb505"
   },
   "source": [
    "## Train the Model\n",
    "Here we train the model and use model checkpointing and early stopping to keep only the best version of the model\n",
    "* ModelCheckpoint -- Callback to save the Keras model or model weights at some frequency.\n",
    "* EarlyStopping -- Stop training when a monitored metric has stopped improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0c81808-bb80-4c72-9150-4d4e9c8fa30f",
    "_uuid": "24747af54fb0253adb9934a78661ff0e68204af0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "\n",
    "weights = \"weights.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(weights, monitor='val_loss', verbose=1, save_best_only = True, mode = 'min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] \n",
    "\n",
    "model.fit(X_train, y, \n",
    "          batch_size = batch_size, \n",
    "          epochs = epochs, \n",
    "          validation_split = 0.1, \n",
    "          callbacks = callbacks_list)\n",
    "\n",
    "model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0659da8855bf9d6ef0ed6cbe03196c598779d905"
   },
   "source": [
    "### Training Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f3312dc2a010cc1ffbb9ee9646f039ed2dc3f4d"
   },
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(X_train, y, batch_size=batch_size)\n",
    "for c_name, c_val in zip(model.metrics_names, eval_results):\n",
    "    print(c_name, '%2.3f' % (c_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model's performance on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_text = pd.read_csv(\"./toxic/test.csv\")\n",
    "test_data_labels = pd.read_csv(\"./toxic/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_data_text[\"comment_text\"].fillna(\"Invalid\").values\n",
    "tokenized_test_text = tokenizer.texts_to_sequences(test_text)\n",
    "X_test = sequence.pad_sequences(tokenized_test_text, maxlen = maxlen)\n",
    "y_test = test_data_labels[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "for c_name, c_val in zip(model.metrics_names, eval_results):\n",
    "    print(c_name, '%2.3f' % (c_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Russian Troll Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07f12751a9cccb7c3f55a1656ac3c4b237b7fa28"
   },
   "source": [
    "Since the files are large we use dask to handle the loading of dataset. We then focus on the tweet itself (content) and the category (account_category) to see if our hate-speech model shows similar results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27afef29b2f47e0bfef77c657527e631d96d6918"
   },
   "outputs": [],
   "source": [
    "rustweet_dir = os.path.join('./', 'Russian Troll')\n",
    "all_tweets_ddf = ddf.read_csv(os.path.join(rustweet_dir, '*.csv'), assume_missing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_ddf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the content and account category for English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27afef29b2f47e0bfef77c657527e631d96d6918"
   },
   "outputs": [],
   "source": [
    "english_tweets_ddf = all_tweets_ddf[all_tweets_ddf['language'].isin(['English'])]\n",
    "content_cat_ddf = english_tweets_ddf[['content', 'account_category']]\n",
    "contents = content_cat_ddf.sample(frac=0.2).compute().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the frequency distribution for different account categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c355b316d618cab364e00531ac0a05e7c64bfcac"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, figsize = (10, 5))\n",
    "contents['account_category'].hist(ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cb5f958bf28b0208a0a12bb9a9b8fe5e3cf20c5"
   },
   "outputs": [],
   "source": [
    "tweets = contents[\"content\"].fillna(\"Invalid\").values\n",
    "tokenized_tweets = tokenizer.texts_to_sequences(tweets)\n",
    "X_tweet = sequence.pad_sequences(tokenized_tweets, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "676283f5a836e87ef4cef0df1b42f2315b63790d"
   },
   "outputs": [],
   "source": [
    "y_tweet = model.predict(X_tweet, batch_size=1024, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Toxicity Dataframe from the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e88c188962e06e9b87268233d038b2b024cff7c"
   },
   "outputs": [],
   "source": [
    "toxicity_df = pd.DataFrame(y_tweet, columns = list_classes)\n",
    "toxicity_df['content_category'] = contents['account_category'].values.copy()\n",
    "toxicity_df['total_hatefulness'] = np.sum(y_tweet, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "415c65a8fb31110d50dab87d67df6561afa953c1"
   },
   "source": [
    "### Example tweet and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c17da83d52e024cb93aeb26675ae310014f382c"
   },
   "outputs": [],
   "source": [
    "def show_sentence(sent_index):\n",
    "    display_markdown('### Input Sentence:\\n `{}`'.format(tweets[sent_index]))\n",
    "    c_pred = model.predict(X_tweet[sent_index : sent_index + 1])[0]\n",
    "    display_markdown('### Scores')\n",
    "    for k, p in zip(list_classes, c_pred):\n",
    "        display_markdown('- {}, Prediction: {:2.2f}%'.format(k, 100*p))\n",
    "show_sentence(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1bbf114c43c132c2a3afa853d6143905831098e"
   },
   "source": [
    "### Identity hate levels of different content categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "132d3f923fd2b0d4fd7fb70a0a06ccc196583c9c"
   },
   "outputs": [],
   "source": [
    "cat_sample_df = toxicity_df.groupby('content_category').apply(lambda x: x.sample(250, replace = False if x.shape[0] > 1000 else True)).reset_index(drop = True)\n",
    "sns.factorplot(y = 'content_category', x = 'identity_hate', kind = 'swarm', data = cat_sample_df, size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e34cf82acb09b35f22e14ad27aa166fea617f581"
   },
   "source": [
    "### Classifying bots based on hate speech scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243151f79ed500fb2d447263846db76a7e78d1c9"
   },
   "outputs": [],
   "source": [
    "tx_train_df, tx_valid_df = train_test_split(toxicity_df, \n",
    "                                            test_size = 0.25,\n",
    "                                            random_state = 2018,\n",
    "                                            stratify=toxicity_df['content_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecffbbdd63fdb27d4ace62021d03497923045b34"
   },
   "outputs": [],
   "source": [
    "def fit_and_show(in_skl_model):\n",
    "    in_skl_model.fit(tx_train_df[list_classes], tx_train_df['content_category'])\n",
    "    out_pred = in_skl_model.predict(tx_valid_df[list_classes])\n",
    "    print('%2.2f%%' % (100*accuracy_score(out_pred, tx_valid_df['content_category'])), 'accuracy')\n",
    "    print(classification_report(out_pred, tx_valid_df['content_category']))\n",
    "    sns.heatmap(confusion_matrix(tx_valid_df['content_category'], out_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d956e3a898f9d5f76332686ffed7d0cb9e1639f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"DUMMY CLASSIFIER\")\n",
    "fit_and_show(dmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d956e3a898f9d5f76332686ffed7d0cb9e1639f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "fit_and_show(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d956e3a898f9d5f76332686ffed7d0cb9e1639f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LOGISTIC REGRESSION\")\n",
    "fit_and_show(lrm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
